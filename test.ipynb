{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Filter 1: remove KeyPhrases Longer than 3 Words\n",
    "def filter1(preds):\n",
    "    final_preds = []\n",
    "    for index, pred in enumerate(preds):\n",
    "\n",
    "\n",
    "        final_preds.append(pred.copy())\n",
    "        for kp in pred:\n",
    "            length = len(kp.split())\n",
    "\n",
    "\n",
    "            if length > 3:\n",
    "\n",
    "\n",
    "                final_preds[index].remove(kp)\n",
    "    return final_preds\n",
    "\n",
    "\n",
    "\n",
    "# Filter 2: Mixing One Words\n",
    "def return_one_words(pred):\n",
    "    ones = []\n",
    "    for kp in pred:\n",
    "        if len(kp.split()) == 1:\n",
    "\n",
    "            ones.append(kp)\n",
    "    return ones\n",
    "\n",
    "\n",
    "\n",
    "def mix_ones(ones):\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    if len(ones) > 1:\n",
    "        for word1 in ones:\n",
    "            for word2 in ones:\n",
    "                if word1 != word2:\n",
    "                    output[word1 + \" \" + word2] = [word1, word2]\n",
    "\n",
    "    else:\n",
    "\n",
    "        output = {None: None}\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def filter2(preds, text_ngrams, word2score, threshold=0.45):\n",
    "    output_preds = []\n",
    "    for index, pred in enumerate(preds):\n",
    "        output_preds.append(pred.copy())\n",
    "        ones = return_one_words(pred)\n",
    "        mixed = mix_ones(ones)\n",
    "\n",
    "\n",
    "        for key in mixed.keys():\n",
    "\n",
    "            if key in text_ngrams[index]:\n",
    "\n",
    "                word1 = mixed[key][0]\n",
    "                word2 = mixed[key][1]\n",
    "                try:\n",
    "                    word1_score = word2score[word1]\n",
    "                    word2_score = word2score[word2]\n",
    "                    candidate_score = word2score[key]\n",
    "\n",
    "                    mean_score = threshold * (word1_score + word2_score)\n",
    "                    if candidate_score >= mean_score:\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            output_preds[index].remove(word1)\n",
    "                            output_preds[index].remove(word2)\n",
    "                            pass\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        if key not in output_preds[index]:\n",
    "\n",
    "                            output_preds[index].append(key)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return output_preds\n",
    "\n",
    "\n",
    "\n",
    "# Filter 3: KeyPhrase Subsets Remove\n",
    "def filter3(preds, word2score):\n",
    "\n",
    "    output_preds = []\n",
    "    for index, pred in enumerate(preds):\n",
    "        output_preds.append(pred.copy())\n",
    "        for small_kp in pred:\n",
    "            small_words = small_kp.split()\n",
    "            for big_kp in pred:\n",
    "                big_words = big_kp.split()\n",
    "\n",
    "                if len(small_words) < len(big_words):\n",
    "                    vectorizer = CountVectorizer(\n",
    "                        ngram_range=(1, len(big_words)))\n",
    "                    vectorizer.fit([big_kp])\n",
    "                    big_ngrams = vectorizer.get_feature_names_out()\n",
    "                    if small_kp in big_ngrams:\n",
    "                        try:\n",
    "                            small_score = word2score[small_kp]\n",
    "                            big_score = word2score[big_kp]\n",
    "                            remove_small = False\n",
    "                            remove_big = False\n",
    "\n",
    "                            if len(small_words) == 2:\n",
    "                                if small_score > 4*big_score:\n",
    "                                    remove_big = True\n",
    "\n",
    "                                elif small_score < 1.8*big_score:\n",
    "                                    remove_small = True\n",
    "\n",
    "                            else:\n",
    "                                if small_score*0.6 <= big_score:\n",
    "                                    remove_small = True\n",
    "\n",
    "                                elif small_score > 4.2*big_score:\n",
    "                                    remove_big = True\n",
    "\n",
    "                            if remove_small:\n",
    "                                output_preds[index].remove(small_kp)\n",
    "                            if remove_big:\n",
    "                                output_preds[index].remove(big_kp)\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "    return output_preds\n",
    "\n",
    "\n",
    "# Filter 4: mixing two words\n",
    "def return_two_words(pred):\n",
    "    twos = []\n",
    "    for kp in pred:\n",
    "        if len(kp.split()) == 2:\n",
    "            twos.append(kp)\n",
    "    return twos\n",
    "\n",
    "def mix_twos(twos):\n",
    "    output = {}\n",
    "    if len(twos) > 1:\n",
    "        for word1 in twos:\n",
    "            for word2 in twos:\n",
    "                if word1 != word2:\n",
    "                    if word1.split()[1] == word2.split()[0]:\n",
    "                        output[word1 + \" \" + word2.split()[1]] = [word1, word2]\n",
    "\n",
    "    else:\n",
    "        output = {None: None}\n",
    "    return output\n",
    "\n",
    "\n",
    "def filter4(preds, word2score, threshold=0.45):\n",
    "    output_preds = []\n",
    "    for index, pred in enumerate(preds):\n",
    "        output_preds.append(pred.copy())\n",
    "        twos = return_two_words(pred)\n",
    "        mixed = mix_twos(twos)\n",
    "        for key, value in mixed.items():\n",
    "\n",
    "            try:\n",
    "\n",
    "                word1 = value[0]\n",
    "                word2 = value[1]\n",
    "                word1_score = word2score[word1]\n",
    "                word2_score = word2score[word2]\n",
    "                mean_score = threshold * (word1_score + word2_score)\n",
    "                candidate_score = word2score[key]\n",
    "                if candidate_score >= mean_score:\n",
    "                    try:\n",
    "                        output_preds[index].remove(word1)\n",
    "                        output_preds[index].remove(word2)\n",
    "                        pass\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    if key not in output_preds[index]:\n",
    "\n",
    "                        output_preds[index].append(key)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    return output_preds\n",
    "\n",
    "\n",
    "\n",
    "def increment_score(kp, score):\n",
    "\n",
    "    length = len(kp.split())\n",
    "    factor1 = 0.045\n",
    "    factor2 = 0.12\n",
    "    factor3 = 0.03\n",
    "\n",
    "    if length == 1:\n",
    "        score += factor1\n",
    "    if length == 2:\n",
    "        score += factor2\n",
    "    if length == 3:\n",
    "        score += factor3\n",
    "    return score\n",
    "\n",
    "def candidate_score(features, tfidf_scores, candidates, page_count):\n",
    "    chunk_indexes = []\n",
    "    chunk_features = []\n",
    "    chunk_scores = []\n",
    "    feature_score = []\n",
    "    for i in range(page_count):\n",
    "        tmp_indexes = []\n",
    "        tmp_features = []\n",
    "        tmp_scores = []\n",
    "        for index, feature in enumerate(features):\n",
    "            if feature in candidates[i]:\n",
    "                tmp_indexes.append(index)\n",
    "                tmp_features.append(feature)\n",
    "                score = tfidf_scores[i][index]\n",
    "                score = increment_score(feature, score)\n",
    "                tmp_scores.append(score)\n",
    "        chunk_indexes.append(tmp_indexes)\n",
    "        chunk_features.append(tmp_features)\n",
    "        chunk_scores.append(tmp_scores)\n",
    "\n",
    "\n",
    "    for i in range(len(chunk_features)):\n",
    "        tmp = []\n",
    "        for feature, score in zip(chunk_features[i], chunk_scores[i]):\n",
    "            tmp.append((feature, score))\n",
    "        feature_score.append(sorted(tmp, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return feature_score\n",
    "\n",
    "\n",
    "def add_NE(name_entities, preds):\n",
    "    for index, name_entity_lst in enumerate(name_entities):\n",
    "        for name_entity in name_entity_lst:\n",
    "            if name_entity not in preds[index]:\n",
    "                preds[index].append(name_entity)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def apply_filters(preds, descrete_ngrams, word2score):\n",
    "    preds = filter1(preds)\n",
    "    preds = filter2(preds, descrete_ngrams, word2score)\n",
    "    preds = filter4(preds, word2score, threshold=0.4)\n",
    "    # preds = filter3(preds, word2score)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def predict_with_threshold(feature_score, descrete_ngrams, word2score, name_entities,\n",
    "                           minimum=4, maximum=6, threshold=0.2):\n",
    "    preds = []\n",
    "    for pred in feature_score:\n",
    "        tmp_pred = []\n",
    "        for feature, score in pred:\n",
    "            if score >= threshold:\n",
    "                tmp_pred.append(feature)\n",
    "        tmp_pred = apply_filters([tmp_pred], descrete_ngrams, word2score)[0]\n",
    "\n",
    "        if len(tmp_pred) < minimum:\n",
    "            tmp_pred = []\n",
    "            for feature, _ in pred[:20]:\n",
    "                tmp_pred.append(feature)\n",
    "            tmp_pred = apply_filters([tmp_pred], descrete_ngrams, word2score)[\n",
    "                0][:minimum]\n",
    "\n",
    "        elif len(tmp_pred) > maximum:\n",
    "            tmp_pred = []\n",
    "            for feature, _ in pred[:40]:\n",
    "                tmp_pred.append(feature)\n",
    "            tmp_pred = apply_filters([tmp_pred], descrete_ngrams, word2score)[\n",
    "                0][:maximum]\n",
    "\n",
    "        preds.append(tmp_pred)\n",
    "    preds = add_NE(name_entities, preds)\n",
    "    preds = apply_filters(preds, descrete_ngrams, word2score)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def increment_score(kp, score):\n",
    "    length = len(kp.split())\n",
    "    factor1 = 0.045\n",
    "    factor2 = 0.12\n",
    "    factor3 = 0.03\n",
    "\n",
    "    if length == 1:\n",
    "        score += factor1\n",
    "\n",
    "    if length == 2:\n",
    "        score += factor2\n",
    "\n",
    "    if length == 3:\n",
    "        score += factor3\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_scores(corpus):\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords.words(\n",
    "\n",
    "        'english'), ngram_range=(1, 3))\n",
    "\n",
    "    tfidf_scores = tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "    features = tfidf.get_feature_names_out()\n",
    "    return features, tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phrase2score(tfidf_scores, tfidf_model):\n",
    "    phrase2score = {}\n",
    "\n",
    "    scores = tfidf_scores.max(axis=0)\n",
    "\n",
    "\n",
    "    for feature, index in tfidf_model.vocabulary_.items():\n",
    "\n",
    "        score = scores[index]\n",
    "\n",
    "        phrase2score[feature] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(corpus):\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords.words(\n",
    "        'english'), ngram_range=(1, 3))\n",
    "    candidates = []\n",
    "    for text in corpus:\n",
    "        page_candidates = []\n",
    "        tfidf.fit([text])\n",
    "        ngrams = tfidf.get_feature_names_out()\n",
    "        for ngram in ngrams:\n",
    "            if not ngram.replace(\" \", \"\").isdigit():\n",
    "                ngram = \" \".join(\n",
    "                    [word for word in ngram.split() if not word.isdigit()])\n",
    "                page_candidates.append(ngram)\n",
    "        candidates.append(page_candidates)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tagged_corpus = []\n",
    "# for text in corpus:\n",
    "#     text_tok = nltk.word_tokenize(text)\n",
    "\n",
    "#     pos_tagged = nltk.pos_tag(text_tok)\n",
    "#     pos_tagged_corpus.append(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_NE(corpus):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    all_NE = []\n",
    "    for text in corpus:\n",
    "        NEs = nlp(text)\n",
    "        all_NE.append(NEs)\n",
    "\n",
    "    NE = []\n",
    "    for NE_set in all_NE:\n",
    "        tmp_entities = []\n",
    "        for entity in NE_set.ents:\n",
    "            entity_text = entity.text\n",
    "            if entity.label_ not in ['CARDINAL', 'PERCENT', 'LAW', 'PERSON'] and len(entity_text) > 3 and len(entity_text.split()) < 5:\n",
    "                # print(entity_text , entity.label_)\n",
    "                tmp_entities.append(entity.text)\n",
    "        NE.append(tmp_entities)\n",
    "    return all_NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_score(features, tfidf_scores):\n",
    "    feature_score = candidate_score(\n",
    "        features, tfidf_scores, candidates, page_count=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = predict_with_threshold(feature_score, candidates, phrase2score, NE,\n",
    "\n",
    "                               minimum=5, maximum=5, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KeyPhraseExtractor import extract\n",
    "from pdfExtractor import extract_text\n",
    "result = extract_text('pdfs\\example1.pdf')\n",
    "\n",
    "corpus = []\n",
    "for page in result.values():\n",
    "    text = \"\".join(page[0])\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = extract(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
